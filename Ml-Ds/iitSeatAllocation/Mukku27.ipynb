{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTEBOOK.\n",
    "import kagglehub\n",
    "breadnbu22er_or_cr_2016_to_2024_path = kagglehub.dataset_download('breadnbu22er/or-cr-2016-to-2024')\n",
    "\n",
    "print('Data source import complete.')\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset\n",
    "file_path = f'{breadnbu22er_or_cr_2016_to_2024_path}/or-cr-2016-to-2024/JEE_Rank_2016_2024.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "df.head()\n",
    "\n",
    "### 1. Data Cleaning and Preprocessing\n",
    "\n",
    "# Check for missing values\n",
    "df.isnull().sum()\n",
    "\n",
    "# Convert 'Opening_Rank' and 'Closing_Rank' to numeric, coercing errors to NaN\n",
    "df['Opening_Rank'] = pd.to_numeric(df['Opening_Rank'], errors='coerce')\n",
    "df['Closing_Rank'] = pd.to_numeric(df['Closing_Rank'], errors='coerce')\n",
    "\n",
    "# Drop rows with NaN values in 'Opening_Rank' or 'Closing_Rank'\n",
    "df.dropna(subset=['Opening_Rank', 'Closing_Rank'], inplace=True)\n",
    "\n",
    "# Verify data types\n",
    "df.dtypes\n",
    "\n",
    "### 2. Exploratory Data Analysis (EDA)\n",
    "\n",
    "# Distribution of Opening and Closing Ranks\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(df['Opening_Rank'], bins=50, color='blue', label='Opening Rank', kde=True)\n",
    "sns.histplot(df['Closing_Rank'], bins=50, color='red', label='Closing Rank', kde=True)\n",
    "plt.title('Distribution of Opening and Closing Ranks')\n",
    "plt.xlabel('Rank')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# **Deduction**: \n",
    "# Most opening and closing ranks are concentrated in the lower range, indicating the popularity of top-ranked institutes.\n",
    "\n",
    "# Correlation heatmap\n",
    "numeric_df = df.select_dtypes(include=[np.number])\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n",
    "\n",
    "# **Deduction**: \n",
    "# Strong correlation observed between opening and closing ranks, indicating that highly ranked institutes tend to remain in demand throughout the counseling process.\n",
    "\n",
    "### Additional EDA\n",
    "\n",
    "# Most Popular Institutes by Year\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.countplot(data=df, x='Institute', order=df['Institute'].value_counts().index, palette='viridis')\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Most Popular Institutes (2016-2024)')\n",
    "plt.show()\n",
    "\n",
    "# **Deduction**:\n",
    "# Certain IITs, such as IIT Bombay and IIT Delhi, dominate the preferences of aspirants.\n",
    "\n",
    "# Popular Branches Over Time\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.countplot(data=df, x='Branch', order=df['Branch'].value_counts().index, palette='coolwarm')\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Most Popular Branches (2016-2024)')\n",
    "plt.show()\n",
    "\n",
    "# **Deduction**: \n",
    "# Computer Science Engineering (CSE) is the most sought-after branch across years, followed by Electronics and Electrical Engineering.\n",
    "\n",
    "# Year-wise Comparison of Opening and Closing Ranks\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.lineplot(data=df, x='Year', y='Opening_Rank', label='Opening Rank', marker='o')\n",
    "sns.lineplot(data=df, x='Year', y='Closing_Rank', label='Closing Rank', marker='o')\n",
    "plt.title('Opening vs Closing Ranks Over the Years')\n",
    "plt.show()\n",
    "\n",
    "# **Deduction**:\n",
    "# Both opening and closing ranks have remained relatively stable over the years, with some fluctuations due to changing preferences.\n",
    "\n",
    "### 3. Predictive Modeling\n",
    "\n",
    "# Predicting the Closing Rank using Random Forest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Prepare the data\n",
    "X = df.drop(columns=['Closing_Rank'])\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "y = df['Closing_Rank']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "\n",
    "# **Inference**: The Random Forest model shows a decent performance with an MAE of {mae}. However, further hyperparameter tuning might be needed to improve accuracy.\n",
    "\n",
    "### 4. Bonus: Predicting Popular Choices\n",
    "\n",
    "# Predicting the most popular branch/institute based on rank\n",
    "# Convert branch/institute to a binary classification problem (popular or not)\n",
    "df['Popular_Choice'] = np.where(df['Closing_Rank'] < 1000, 1, 0)  # Assuming top 1000 ranks indicate popularity\n",
    "\n",
    "# Prepare data for classification\n",
    "X = df.drop(columns=['Popular_Choice', 'Closing_Rank', 'Opening_Rank'])\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "y = df['Popular_Choice']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a classifier (Random Forest)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(random_state=42)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = classifier.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# **Inference**:\n",
    "# The Random Forest Classifier achieves a good accuracy, suggesting that it can predict the likelihood of a branch or institute being popular based on rank.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
